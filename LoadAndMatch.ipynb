{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\marti\\anaconda3\\Lib\\site-packages\\paramiko\\transport.py:219: CryptographyDeprecationWarning: Blowfish has been deprecated\n",
      "  \"class\": algorithms.Blowfish,\n"
     ]
    }
   ],
   "source": [
    "## import numpy as np # linear algebra\n",
    "from matplotlib import pyplot as plt\n",
    "#We need these in this file:\n",
    "import sys\n",
    "sys.path.append('cropping')\n",
    "sys.path.append('scripts')\n",
    "import load_read_name_extractor as lrne\n",
    "import SVM_classifier_general as svm_general\n",
    "import DeepFunctions as df\n",
    "\n",
    "#Taget fra l√¶ngere nede i koden:\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "from torchvision import transforms\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from PIL import Image\n",
    "import random\n",
    "import itertools\n",
    "import numpy as np\n",
    "from random import shuffle\n",
    "from IPython.display import display\n",
    "import os\n",
    "import json\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Remember to change the cuda:0 value. Should match a free GPU on the cluster or if using local probably just cuda:0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of available GPUs: 2\n",
      "GPU 0: NVIDIA GeForce RTX 2070 SUPER\n",
      "GPU 1: NVIDIA GeForce GTX 1050 Ti\n",
      "cuda:0\n"
     ]
    }
   ],
   "source": [
    "# Call the function to list GPU names\n",
    "df.list_gpu_names()\n",
    "#R\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainDataloader = torch.load('data/train_loader.pth')\n",
    "valDataloader = torch.load('data/val_loader.pth')\n",
    "testDataloader = torch.load('data/test_loader.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "lenthdataset = torch.load('data/lengthsdataset.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "117\n",
      "10\n",
      "10\n"
     ]
    }
   ],
   "source": [
    "print(len(trainDataloader))\n",
    "print(len(valDataloader))\n",
    "print(len(testDataloader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#We have ignored class 9 since there were only 1 image with that class. Therfore we have 9 classes.\n",
    "num_classes = 9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResidualBlock(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, kernel_size=3,stride=1):\n",
    "        super(ResidualBlock, self).__init__()\n",
    "\n",
    "        # Define the convolutional layers\n",
    "        self.conv1 = nn.Conv2d(in_channels, out_channels, kernel_size=kernel_size, stride=stride, padding=1, bias=False)\n",
    "        self.bn1 = nn.BatchNorm2d(out_channels)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.conv2 = nn.Conv2d(out_channels, out_channels, kernel_size=kernel_size, stride=stride, padding=1, bias=False)\n",
    "        self.bn2 = nn.BatchNorm2d(out_channels)\n",
    "\n",
    "        # Shortcut connection (identity mapping)\n",
    "        self.shortcut = nn.Sequential()\n",
    "        if stride != 1 or in_channels != out_channels:\n",
    "            self.shortcut = nn.Sequential(\n",
    "                nn.Conv2d(in_channels, out_channels, kernel_size=1, stride=stride, bias=False),\n",
    "                nn.BatchNorm2d(out_channels)\n",
    "            )\n",
    "\n",
    "    def forward(self, x):\n",
    "        residual = x\n",
    "\n",
    "        out = self.conv1(x)\n",
    "        out = self.bn1(out)\n",
    "        out = self.relu(out)\n",
    "\n",
    "        out = self.conv2(out)\n",
    "        out = self.bn2(out)\n",
    "\n",
    "        # Add the shortcut connection\n",
    "        out += self.shortcut(residual)\n",
    "        out = self.relu(out)\n",
    "\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "kernel_size = 3\n",
    "padding = 0\n",
    "# define network\n",
    "conv1toconv2 = 16\n",
    "conv2toconv3 = 32\n",
    "conv3toconv4 = 64\n",
    "conv4toconv5 = 128\n",
    "conv5tores1 = 256\n",
    "res1tores2 = 256\n",
    "res2tores3 = 256\n",
    "res3tores4 = 256\n",
    "res4tores5 = 256\n",
    "res5tores6 = 256\n",
    "res6tolin1 = 256\n",
    "lin = 256\n",
    "\n",
    "class NeuralNetwork(nn.Module):\n",
    "    def __init__(self, dropout_rate=0.5, momentum=0.1):\n",
    "        super(NeuralNetwork, self).__init__()\n",
    "        \n",
    "        self.conv1 = nn.Conv2d(3, conv1toconv2, kernel_size, padding=padding)\n",
    "        self.bn1 = nn.BatchNorm2d(conv1toconv2, momentum = momentum)\n",
    "        self.conv2 = nn.Conv2d(conv1toconv2, conv2toconv3, kernel_size, padding=padding)\n",
    "        self.bn2 = nn.BatchNorm2d(conv2toconv3)\n",
    "        self.conv3 = nn.Conv2d(conv2toconv3, conv3toconv4, kernel_size, padding=padding)\n",
    "        self.bn3 = nn.BatchNorm2d(conv3toconv4)\n",
    "        self.conv4 = nn.Conv2d(conv3toconv4, conv4toconv5, kernel_size, padding=padding)\n",
    "        self.bn4 = nn.BatchNorm2d(conv4toconv5)\n",
    "        self.conv5 = nn.Conv2d(conv4toconv5, conv5tores1, kernel_size, padding=padding)\n",
    "        self.bn5 = nn.BatchNorm2d(conv5tores1)\n",
    "        \n",
    "        self.residual1 = ResidualBlock(conv5tores1, res1tores2)\n",
    "        self.residual2 = ResidualBlock(res1tores2, res2tores3)\n",
    "        self.residual3 = ResidualBlock(res2tores3, res3tores4)\n",
    "        self.residual4 = ResidualBlock(res4tores5, res4tores5)\n",
    "        self.residual5 = ResidualBlock(res4tores5, res5tores6)\n",
    "        self.residual6 = ResidualBlock(res5tores6, res6tolin1)\n",
    "        \n",
    "        self.pool = nn.AvgPool2d(2, 2)\n",
    "        self.poolmax = nn.MaxPool2d(2,2)\n",
    "        self.flatten = nn.Flatten()\n",
    "        self.FC1 = nn.Linear(6 * res6tolin1, lin)\n",
    "        self.bn7 = nn.BatchNorm1d(lin)\n",
    "        self.FC2 = nn.Linear(lin,lin)\n",
    "        self.bn8 = nn.BatchNorm1d(lin)\n",
    "        self.FC3 = nn.Linear(lin, num_classes)\n",
    "        self.bn9 = nn.BatchNorm1d(256)\n",
    "        self.dropout = nn.Dropout(dropout_rate)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = self.bn1(x)\n",
    "        x = F.relu(x)\n",
    "        #print(x.shape)\n",
    "        x = self.poolmax(x)\n",
    "        \n",
    "        #print(x.shape)\n",
    "        x = self.conv2(x)\n",
    "        x = self.bn2(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.pool(x)\n",
    "        \n",
    "        #print(x.shape)\n",
    "        x = self.conv3(x)\n",
    "        x = self.bn3(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.pool(x)\n",
    "        \n",
    "                                  \n",
    "        x = self.conv4(x)\n",
    "        x = self.bn4(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.pool(x)\n",
    "        #print(x.shape)\n",
    "        \n",
    "        x = self.conv5(x)\n",
    "        x = self.bn5(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.pool(x)\n",
    "        #print(x.shape)\n",
    "        \n",
    "        x = self.residual1(x)\n",
    "        x = self.residual2(x)\n",
    "        x = self.residual3(x)\n",
    "        x = self.residual4(x)\n",
    "        x = self.residual5(x)\n",
    "        x = self.residual6(x)\n",
    "        \n",
    "        x = self.pool(x)\n",
    "        \n",
    "        x = self.flatten(x)\n",
    "        x = self.FC1(x)\n",
    "        x = self.bn7(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.dropout(x)\n",
    "        \n",
    "        x = self.FC2(x)\n",
    "        x = self.bn8(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.dropout(x)\n",
    "        \n",
    "        x = self.FC3(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "start\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1] train loss: 2.236 train acc: 0.197 val loss: 2.372 val acc: 0.234 \n",
      "[2] train loss: 2.181 train acc: 0.264 val loss: 2.191 val acc: 0.232 \n",
      "[3] train loss: 2.154 train acc: 0.300 val loss: 2.193 val acc: 0.215 \n",
      "[4] train loss: 2.143 train acc: 0.311 val loss: 2.177 val acc: 0.255 \n",
      "[5] train loss: 2.124 train acc: 0.338 val loss: 2.951 val acc: 0.171 \n",
      "[6] train loss: 2.113 train acc: 0.359 val loss: 2.270 val acc: 0.194 \n",
      "[7] train loss: 2.102 train acc: 0.361 val loss: 2.339 val acc: 0.174 \n",
      "[8] train loss: 2.087 train acc: 0.387 val loss: 3.012 val acc: 0.113 \n",
      "[9] train loss: 2.077 train acc: 0.395 val loss: 2.318 val acc: 0.219 \n",
      "[10] train loss: 2.067 train acc: 0.418 val loss: 2.767 val acc: 0.171 \n",
      "[11] train loss: 2.062 train acc: 0.420 val loss: 3.611 val acc: 0.042 \n",
      "Epoch 00012: reducing learning rate of group 0 to 5.0000e-06.\n",
      "[12] train loss: 2.051 train acc: 0.440 val loss: 2.347 val acc: 0.218 \n",
      "[13] train loss: 2.047 train acc: 0.444 val loss: 2.211 val acc: 0.227 \n",
      "[14] train loss: 2.036 train acc: 0.458 val loss: 2.249 val acc: 0.184 \n",
      "[15] train loss: 2.027 train acc: 0.467 val loss: 2.183 val acc: 0.245 \n",
      "[16] train loss: 2.024 train acc: 0.480 val loss: 2.200 val acc: 0.223 \n",
      "[17] train loss: 2.019 train acc: 0.486 val loss: 2.172 val acc: 0.258 \n",
      "[18] train loss: 2.015 train acc: 0.492 val loss: 2.316 val acc: 0.210 \n",
      "[19] train loss: 2.013 train acc: 0.505 val loss: 2.220 val acc: 0.244 \n",
      "[20] train loss: 2.003 train acc: 0.510 val loss: 2.144 val acc: 0.295 \n",
      "[21] train loss: 2.002 train acc: 0.517 val loss: 2.200 val acc: 0.213 \n",
      "[22] train loss: 1.995 train acc: 0.529 val loss: 2.346 val acc: 0.182 \n",
      "[23] train loss: 1.989 train acc: 0.538 val loss: 2.183 val acc: 0.253 \n"
     ]
    }
   ],
   "source": [
    "#%%capture output\n",
    "done=False\n",
    "learning_rates = [0.00001]\n",
    "patience_values = [7]\n",
    "weight_decay_values =[0]\n",
    "label_smoothing_values =[0.5]\n",
    "factor_values=[0.5]\n",
    "message = \"Standard\"\n",
    "\n",
    "# Set the number of epochs\n",
    "num_epochs = 200\n",
    "totalAcc = 0\n",
    "\n",
    "trainAccList = []\n",
    "valAccList = []\n",
    "trainLossList = []\n",
    "valLossList = []\n",
    "\n",
    "# Variables for early stopping\n",
    "\n",
    "best_epoch = 0\n",
    "best_model_state = None\n",
    "filename = None\n",
    "start_scheduler_epoch = 0\n",
    "momentum = 0.1\n",
    "# Training loop + tuning\n",
    "for learning_rate in learning_rates:\n",
    "    for weight_decay in weight_decay_values:\n",
    "        for patience in patience_values:\n",
    "            for label_smoothing in label_smoothing_values:\n",
    "                for factor in factor_values:\n",
    "                    net = NeuralNetwork(momentum=momentum)\n",
    "                    print(\"start\")\n",
    "                    net.to(device)\n",
    "                    #print(net)\n",
    "                    #learning_rate = 0.001\n",
    "                    # Define the optimizer\n",
    "                    criterion = nn.CrossEntropyLoss(label_smoothing=label_smoothing).to(device)\n",
    "                    optimizer = optim.Adam(net.parameters(), lr=learning_rate, weight_decay=weight_decay)\n",
    "                    scheduler = ReduceLROnPlateau(optimizer, 'min', patience=patience, verbose=True, factor=factor)\n",
    "                    best_val_acc = 0.0\n",
    "                    trainAccList = []\n",
    "                    valAccList = []\n",
    "                    trainLossList = []\n",
    "                    valLossList = []\n",
    "                    # Training loop\n",
    "                    for epoch in range(num_epochs):\n",
    "                        running_loss = 0.0\n",
    "                        totalAcc = 0.0\n",
    "                        val_running_loss = 0.0\n",
    "                        val_totalAcc = 0.0\n",
    "                        true_labels = []\n",
    "                        predicted_labels = []\n",
    "                        best_model_state_in = net.state_dict().copy()\n",
    "                        for i, data in enumerate(trainDataloader, 0):\n",
    "                            net.train()\n",
    "                            # Get the inputs and labels\n",
    "                            inputs, labels = data\n",
    "\n",
    "                            inputs, labels = inputs.to(device), labels.to(device)\n",
    "\n",
    "                            # Zero the parameter gradients\n",
    "                            optimizer.zero_grad()\n",
    "\n",
    "                            # Forward pass\n",
    "                            outputs = net(inputs)\n",
    "                            # Move tensors to CPU before extending lists\n",
    "                            _, predicted = torch.max(outputs, 1)  # Assuming a classification task with softmax activation\n",
    "                            true_labels.extend(labels.cpu().numpy())\n",
    "                            predicted_labels.extend(predicted.cpu().numpy())\n",
    "                            loss = criterion(outputs, labels)\n",
    "\n",
    "                            # Backward pass and optimization\n",
    "                            loss.backward()\n",
    "                            optimizer.step()\n",
    "\n",
    "                            # Print statistics\n",
    "                            running_loss += loss.item()\n",
    "                            del(inputs)\n",
    "                            del(labels)\n",
    "                            # Convert lists to NumPy arrays\n",
    "                        true_labels = np.array(true_labels)\n",
    "                        predicted_labels = np.array(predicted_labels)\n",
    "                        totalAcc = accuracy_score(true_labels, predicted_labels)\n",
    "\n",
    "                        true_labels = []\n",
    "                        predicted_labels = []\n",
    "                        for i, data in enumerate(valDataloader, 0):\n",
    "                            net.eval()\n",
    "                            # Get the inputs and labels\n",
    "                            inputs, labels = data\n",
    "                            inputs, labels = inputs.to(device), labels.to(device)\n",
    "                            # Forward pass\n",
    "                            outputs = net(inputs)\n",
    "                            _, predicted = torch.max(outputs, 1)  # Assuming a classification task with softmax activation\n",
    "                            # Move tensors to CPU before extending lists\n",
    "                            loss = criterion(outputs, labels)\n",
    "\n",
    "                            # Print statistics\n",
    "                            val_running_loss += loss.item()\n",
    "                            true_labels.extend(labels.cpu().numpy())\n",
    "                            predicted_labels.extend(predicted.cpu().numpy())\n",
    "                            del(inputs)\n",
    "                            del(labels)\n",
    "                            \n",
    "                            # Convert lists to NumPy arrays\n",
    "                        true_labels = np.array(true_labels)\n",
    "                        predicted_labels = np.array(predicted_labels)\n",
    "                        val_totalAcc = accuracy_score(true_labels, predicted_labels)\n",
    "                            \n",
    "                        if epoch >= start_scheduler_epoch:\n",
    "                            scheduler.step(val_running_loss)\n",
    "                        \n",
    "\n",
    "                        trainAccList.append(totalAcc)\n",
    "                        valAccList.append(val_totalAcc)\n",
    "                        trainLossList.append(running_loss/len(trainDataloader))\n",
    "                        valLossList.append(val_running_loss/len(valDataloader))\n",
    "                        print('[%d] train loss: %.3f train acc: %.3f val loss: %.3f val acc: %.3f '\n",
    "                              % (epoch + 1,trainLossList[epoch],  trainAccList[epoch], valLossList[epoch], valAccList[epoch]))\n",
    "                        running_loss = 0.0\n",
    "                        totalAcc = 0.0\n",
    "                        val_running_loss = 0.0\n",
    "                        val_totalAcc = 0.0\n",
    "                        current_learning_rate = optimizer.param_groups[0]['lr']\n",
    "\n",
    "                        # Check for improvement in validation accuracy\n",
    "\n",
    "                        if valAccList[epoch] > best_val_acc and valAccList[epoch] > 0.45:\n",
    "                            best_val_acc = valAccList[epoch]\n",
    "                            best_model_state = best_model_state_in \n",
    "                            best_model_state1 = net.state_dict().copy()\n",
    "                            best_epoch = epoch\n",
    "                            best_epoch1 = epoch\n",
    "                            torch.save(best_model_state, 'models/best_model_stateAcc%.3fepoch%.1f.pt' % (best_val_acc, best_epoch))\n",
    "                            torch.save(best_model_state1, 'models/best_model_stateAcc%.3fepoch%.1fex.pt' % (best_val_acc, best_epoch1))\n",
    "\n",
    "\n",
    "                        # Break if totalAcc is above 0.95\n",
    "\n",
    "                    if best_model_state is not None:\n",
    "                        net.load_state_dict(best_model_state)\n",
    "                        filename = 'models/best_model_stateAcc%.3fepoch%.1f.pt' % (best_val_acc, best_epoch)\n",
    "                    best_val_accuracy = 0.0\n",
    "                    best_epoch = 0\n",
    "                    best_model_state = None\n",
    "\n",
    "\n",
    "                    print('Finished training')\n",
    "                    df.save_training_data_to_file(df.transfer_to_cpu([learning_rate,weight_decay,patience, momentum,label_smoothing,factor,filename]),df.transfer_to_cpu(trainLossList)\n",
    "                                                      ,df.transfer_to_cpu(trainAccList),df.transfer_to_cpu(valLossList)\n",
    "                                                      ,df.transfer_to_cpu(valAccList))\n",
    "done=True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#if done:\n",
    "#\toutput.show() # displays captured output\n",
    "#else:\n",
    "#\tprint(\"cell above still seem to be running, wait some more..\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
