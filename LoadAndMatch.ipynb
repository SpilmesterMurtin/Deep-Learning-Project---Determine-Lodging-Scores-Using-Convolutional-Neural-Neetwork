{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## import numpy as np # linear algebra\n",
    "from matplotlib import pyplot as plt\n",
    "#We need these in this file:\n",
    "import sys\n",
    "sys.path.append('cropping')\n",
    "import load_read_name_extractor as lrne\n",
    "import SVM_classifier_general as svm_general\n",
    "\n",
    "#Taget fra l√¶ngere nede i koden:\n",
    "import torch\n",
    "from torchvision import transforms\n",
    "from PIL import Image\n",
    "import random\n",
    "import itertools\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score\n",
    "import numpy as np\n",
    "import DeepFunctions as df\n",
    "from random import shuffle\n",
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "from sklearn.model_selection import train_test_split\n",
    "from torchvision import transforms\n",
    "from IPython.display import display\n",
    "import os\n",
    "import json\n",
    "import torch.optim as optim\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_list, names = lrne.load_name_and_img(\"cropped_mean/*jpeg\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(img_list))\n",
    "print(len(names))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pick a random index\n",
    "random_index = random.randint(0, len(img_list)-1)\n",
    "\n",
    "# get the corresponding image and name\n",
    "random_img = img_list[random_index]\n",
    "random_name = names[random_index]\n",
    "\n",
    "# show the image and print the name\n",
    "plt.imshow(random_img)\n",
    "plt.show()\n",
    "print(random_name)\n",
    "\n",
    "# show the first image and print the name\n",
    "plt.imshow(img_list[0])\n",
    "plt.show()\n",
    "print(names[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "date_ = False\n",
    "bin_size = 10\n",
    "n_bins = int(np.floor(100/bin_size))\n",
    "labels = svm_general.load_labels(bin_size, date_)\n",
    "print(labels[0])\n",
    "print(len(labels))\n",
    "\n",
    "#convert names into [flight_folder, ROI]\n",
    "names_labels = lrne.it_name_extract_labels_from_img_jpeg(names)\n",
    "print(names_labels[0])\n",
    "lodging_score = [[] for _ in range(4915)]\n",
    "\n",
    "#match labels with feature names\n",
    "lodging_score = svm_general.match_pic_label_to_names_new(lodging_score, labels, names_labels, date_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lodging_score_int = [[int(char) for char in sublist] for sublist in lodging_score]\n",
    "flattened_list = list(itertools.chain.from_iterable(lodging_score_int))\n",
    "#print(flattened_list)\n",
    "lodging_score = flattened_list\n",
    "#print(max(lodging_score_int))\n",
    "count_9 = lodging_score.count(9)\n",
    "print(count_9)\n",
    "\n",
    "print(np.shape(img_list))\n",
    "print(np.shape(flattened_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from collections import Counter\n",
    "\n",
    "class_counts = Counter(lodging_score)\n",
    "\n",
    "# Create a bar plot\n",
    "plt.bar(class_counts.keys(), class_counts.values(), color='skyblue')\n",
    "\n",
    "# Add labels and title\n",
    "plt.xlabel('Classes (0-8)')\n",
    "plt.ylabel('Count')\n",
    "plt.title('Class Distribution Bar Plot')\n",
    "\n",
    "# Display the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Remember to change the cuda:0 value. Should match a free GPU on the cluster or if using local probably just cuda:0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Call the function to list GPU names\n",
    "df.list_gpu_names()\n",
    "#R\n",
    "device = torch.device(\"cuda:3\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ImageDataset(Dataset):\n",
    "    def __init__(self, img_list, lodging_score, transform=None):\n",
    "        self.img_list = img_list\n",
    "        self.lodging_score = lodging_score\n",
    "        self.transform = transform\n",
    "        self.mean = 0\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.img_list)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        img = self.img_list[idx]\n",
    "        score = self.lodging_score[idx]\n",
    "\n",
    "        # Convert image to tensor and apply transformations\n",
    "        if self.transform:\n",
    "            img = self.transform(img)\n",
    "\n",
    "        return img, score\n",
    "    def get_labels(self):\n",
    "        return self.lodging_score\n",
    "\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToPILImage(),\n",
    "    transforms.Resize((144, 496)),\n",
    "    #transforms.Grayscale(),\n",
    "    transforms.ToTensor(),\n",
    "    #transforms.Normalize(mean=[0.485], std=[0.229]),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Combine img_list and lodging_score\n",
    "combined_data = list(zip(img_list, lodging_score))\n",
    "\n",
    "# Shuffle the combined data\n",
    "shuffle(combined_data)\n",
    "\n",
    "# Unzip the shuffled data\n",
    "img_list, lodging_score = zip(*combined_data)\n",
    "dataSet = ImageDataset(img_list, lodging_score, transform=transform)\n",
    "print(dataSet[0][1])\n",
    "count_0 = lodging_score.count(0)\n",
    "print(count_0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataSet = df.filter_data(dataSet)\n",
    "print(len(dataSet[0]))\n",
    "print(len(dataSet))\n",
    "print(dataSet[0][0].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Usage example\n",
    "augmented_dataSet = df.augment_data(dataSet)\n",
    "dataSet = augmented_dataSet\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assuming dataSet is a list of tuples where the second element contains the classes\n",
    "class_list = [item[1] for item in augmented_dataSet]\n",
    "\n",
    "# Count occurrences of each class\n",
    "class_counts = Counter(class_list)\n",
    "\n",
    "# Create a bar plot\n",
    "plt.bar(class_counts.keys(), class_counts.values(), color='skyblue')\n",
    "\n",
    "# Add labels and title\n",
    "plt.xlabel('Classes (0-8)')\n",
    "plt.ylabel('Count')\n",
    "plt.title('Class Distribution Bar Plot')\n",
    "\n",
    "# Display the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(dataSet[0]))\n",
    "print(len(dataSet))\n",
    "#print(type(dataSet[0][0]))\n",
    "\n",
    "\n",
    "# Assuming img is a PIL.Image.Image object\n",
    "display(dataSet[0][0].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Split into train and validation set\n",
    "random_seed = 10587\n",
    "testValSize = (2/5) * len(dataSet)\n",
    "trainDataset, testvalDataset = train_test_split(dataSet, test_size=int(testValSize), random_state=random_seed, shuffle=True)\n",
    "validationSize = (1/2) * len(testvalDataset)\n",
    "testDataset, valDataset = train_test_split(testvalDataset, test_size=int(validationSize), random_state=random_seed, shuffle=True)\n",
    "\n",
    "#Create dataloaders\n",
    "batchSize = 64\n",
    "trainDataloader = DataLoader(trainDataset, batch_size=batchSize, shuffle=True)\n",
    "testDataloader = DataLoader(testDataset, batch_size=batchSize, shuffle=True)\n",
    "valDataloader = DataLoader(valDataset, batch_size=batchSize, shuffle=True)\n",
    "\n",
    "#Print sizes\n",
    "print(len(trainDataset), len(testDataset), len(valDataset))\n",
    "print(dataSet[0][0].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "#We have ignored class 9 since there were only 1 image with that class. Therfore we have 9 classes.\n",
    "num_classes = 9\n",
    "channels = dataSet[0][0].shape[0]\n",
    "height = dataSet[0][0].shape[1]\n",
    "width = dataSet[0][0].shape[2]\n",
    "print(channels)\n",
    "print(height)\n",
    "print(width)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class ParallelResidualBlock(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels):\n",
    "        super(ParallelResidualBlock, self).__init__()\n",
    "\n",
    "        # First path: Identity path\n",
    "        self.identity_path = nn.Sequential(\n",
    "            nn.Conv2d(in_channels, out_channels, kernel_size=1, stride=1, padding=0),\n",
    "            nn.BatchNorm2d(out_channels),\n",
    "            nn.ReLU(inplace=True)\n",
    "        )\n",
    "\n",
    "        # Second path: Convolutional path\n",
    "        self.conv_path = nn.Sequential(\n",
    "            nn.Conv2d(in_channels, out_channels, kernel_size=3, stride=1, padding=1),\n",
    "            nn.BatchNorm2d(out_channels),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(out_channels, out_channels, kernel_size=3, stride=1, padding=1),\n",
    "            nn.BatchNorm2d(out_channels),\n",
    "            nn.ReLU(inplace=True)\n",
    "        )\n",
    "\n",
    "        # Third path: Another Convolutional path\n",
    "        self.another_conv_path = nn.Sequential(\n",
    "            nn.Conv2d(in_channels, out_channels, kernel_size=3, stride=1, padding=1),\n",
    "            nn.BatchNorm2d(out_channels),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(out_channels, out_channels, kernel_size=3, stride=1, padding=1),\n",
    "            nn.BatchNorm2d(out_channels),\n",
    "            nn.ReLU(inplace=True)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        identity_output = self.identity_path(x)\n",
    "        conv_output = self.conv_path(x)\n",
    "        another_conv_output = self.another_conv_path(x)\n",
    "\n",
    "        # Combine the outputs\n",
    "        output = identity_output + conv_output + another_conv_output\n",
    "\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResidualBlock(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, kernel_size=3,stride=1):\n",
    "        super(ResidualBlock, self).__init__()\n",
    "\n",
    "        # Define the convolutional layers\n",
    "        self.conv1 = nn.Conv2d(in_channels, out_channels, kernel_size=kernel_size, stride=stride, padding=1, bias=False)\n",
    "        self.bn1 = nn.BatchNorm2d(out_channels)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.conv2 = nn.Conv2d(out_channels, out_channels, kernel_size=kernel_size, stride=stride, padding=1, bias=False)\n",
    "        self.bn2 = nn.BatchNorm2d(out_channels)\n",
    "\n",
    "        # Shortcut connection (identity mapping)\n",
    "        self.shortcut = nn.Sequential()\n",
    "        if stride != 1 or in_channels != out_channels:\n",
    "            self.shortcut = nn.Sequential(\n",
    "                nn.Conv2d(in_channels, out_channels, kernel_size=1, stride=stride, bias=False),\n",
    "                nn.BatchNorm2d(out_channels)\n",
    "            )\n",
    "\n",
    "    def forward(self, x):\n",
    "        residual = x\n",
    "\n",
    "        out = self.conv1(x)\n",
    "        out = self.bn1(out)\n",
    "        out = self.relu(out)\n",
    "\n",
    "        out = self.conv2(out)\n",
    "        out = self.bn2(out)\n",
    "\n",
    "        # Add the shortcut connection\n",
    "        out += self.shortcut(residual)\n",
    "        out = self.relu(out)\n",
    "\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kernel_size = 3\n",
    "kernel_size1 = 3\n",
    "padding = 0\n",
    "# define network\n",
    "conv1toconv2 = 64\n",
    "conv2toconv3 = 128\n",
    "conv3toconv4 = 128\n",
    "conv4tores1 = 128\n",
    "res1tores2 = 128\n",
    "res2tores3 = 128\n",
    "res3tores4 = 128\n",
    "res4tores5 = 128\n",
    "res5tores6 = 128\n",
    "res6tores7 = 128\n",
    "res7tores8 = 128\n",
    "res8tolin1 = 128\n",
    "\n",
    "class NeuralNetwork(nn.Module):\n",
    "    def __init__(self, dropout_rate=0.5):\n",
    "        super(NeuralNetwork, self).__init__()\n",
    "        \n",
    "        self.conv1 = nn.Conv2d(3, conv1toconv2, kernel_size, padding=padding)\n",
    "        self.bn1 = nn.BatchNorm2d(conv1toconv2)\n",
    "        self.conv2 = nn.Conv2d(conv1toconv2, conv2toconv3, kernel_size, padding=padding)\n",
    "        self.bn2 = nn.BatchNorm2d(conv2toconv3)\n",
    "        self.conv3 = nn.Conv2d(conv2toconv3, conv3toconv4, kernel_size, padding=padding)\n",
    "        self.bn3 = nn.BatchNorm2d(conv3toconv4)\n",
    "        self.conv4 = nn.Conv2d(conv3toconv4, conv4tores1, kernel_size, padding=padding)\n",
    "        self.bn4 = nn.BatchNorm2d(conv4tores1)\n",
    "        \n",
    "        self.residual1 = ResidualBlock(conv4tores1, res1tores2)\n",
    "        self.residual2 = ResidualBlock(res1tores2, res2tores3)\n",
    "        self.residual3 = ResidualBlock(res2tores3, res3tores4)\n",
    "        self.residual4 = ResidualBlock(res4tores5, res4tores5)\n",
    "        self.residual5 = ResidualBlock(res4tores5, res5tores6)\n",
    "        self.residual6 = ResidualBlock(res5tores6, res6tores7)\n",
    "        self.residual7 = ResidualBlock(res6tores7, res7tores8)\n",
    "        self.residual8 = ResidualBlock(res7tores8, res8tolin1)\n",
    "        \n",
    "        self.pool = nn.AvgPool2d(2, 2)\n",
    "        self.flatten = nn.Flatten()\n",
    "        self.FC1 = nn.Linear(7 * 29 * res8tolin1, 256)\n",
    "        self.bn7 = nn.BatchNorm1d(256)\n",
    "        self.FC2 = nn.Linear(256, 512)\n",
    "        self.bn8 = nn.BatchNorm1d(512)\n",
    "        self.FC3 = nn.Linear(512, 256)\n",
    "        self.bn9 = nn.BatchNorm1d(256)\n",
    "        self.FC4 = nn.Linear(256, num_classes)\n",
    "        self.dropout = nn.Dropout(dropout_rate)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = self.bn1(x)\n",
    "        x = F.relu(x)\n",
    "        #print(x.shape)\n",
    "        x = self.pool(x)\n",
    "        \n",
    "        #print(x.shape)\n",
    "        x = self.conv2(x)\n",
    "        x = self.bn2(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.pool(x)\n",
    "        \n",
    "        #print(x.shape)\n",
    "        x = self.conv3(x)\n",
    "        x = self.bn3(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.pool(x)\n",
    "        \n",
    "                                  \n",
    "        x = self.conv4(x)\n",
    "        x = self.bn4(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.pool(x)\n",
    "        #print(x.shape)\n",
    "        \n",
    "        self.residual1(x)\n",
    "        self.residual2(x)\n",
    "        self.residual3(x)\n",
    "        self.residual4(x)\n",
    "        self.residual5(x)\n",
    "        self.residual6(x)\n",
    "        self.residual7(x)\n",
    "        self.residual8(x)\n",
    "        \n",
    "        x = self.flatten(x)\n",
    "        x = self.FC1(x)\n",
    "        x = self.bn7(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.dropout(x)\n",
    "        \n",
    "        x = self.FC2(x)\n",
    "        x = self.bn8(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.dropout(x)\n",
    "        \n",
    "        x = self.FC3(x)\n",
    "        x = self.bn9(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.FC4(x)\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture output\n",
    "done=False\n",
    "learning_rates = [0.001, 0.00025,0.0005, 0.00075,0.0001]\n",
    "patience_values = [5, 10,100]\n",
    "weight_decay_values =[0,1e-3,1e-5]\n",
    "best_val_acc = 0.0\n",
    "best_hyperparameters = {}\n",
    "\n",
    "# Set the number of epochs\n",
    "num_epochs = 80\n",
    "totalAcc = 0\n",
    "\n",
    "trainAccList = []\n",
    "valAccList = []\n",
    "trainLossList = []\n",
    "valLossList = []\n",
    "\n",
    "# Variables for early stopping\n",
    "best_val_accuracy = 0.0\n",
    "patience = 100  # Number of epochs to wait for improvement\n",
    "counter = 0  # Counter for consecutive epochs without improvement\n",
    "\n",
    "\n",
    "# Training loop + tuning\n",
    "for learning_rate in learning_rates:\n",
    "    for weight_decay in weight_decay_values:\n",
    "        for patience in patience_values:\n",
    "            net = NeuralNetwork()\n",
    "            print(\"start\")\n",
    "            net.to(device)\n",
    "            #print(net)\n",
    "            #learning_rate = 0.001\n",
    "            # Define the optimizer\n",
    "            criterion = nn.CrossEntropyLoss(label_smoothing=0.2).to(device)\n",
    "            optimizer = optim.Adam(net.parameters(), lr=learning_rate, weight_decay=weight_decay)\n",
    "            scheduler = ReduceLROnPlateau(optimizer, 'min', patience=patience, verbose=True)\n",
    "\n",
    "            trainAccList = []\n",
    "            valAccList = []\n",
    "            trainLossList = []\n",
    "            valLossList = []\n",
    "            # Training loop\n",
    "            for epoch in range(num_epochs):\n",
    "                running_loss = 0.0\n",
    "                totalAcc = 0.0\n",
    "                val_running_loss = 0.0\n",
    "                val_totalAcc = 0.0\n",
    "\n",
    "                for i, data in enumerate(trainDataloader, 0):\n",
    "                    net.train()\n",
    "                    # Get the inputs and labels\n",
    "                    inputs, labels = data\n",
    "\n",
    "                    inputs, labels = inputs.to(device), labels.to(device)\n",
    "\n",
    "                    # Zero the parameter gradients\n",
    "                    optimizer.zero_grad()\n",
    "\n",
    "                    # Forward pass\n",
    "                    outputs = net(inputs)\n",
    "                    totalAcc += torch.eq(torch.argmax(outputs, dim=1), labels).sum()\n",
    "\n",
    "                    loss = criterion(outputs, labels)\n",
    "\n",
    "                    # Backward pass and optimization\n",
    "                    loss.backward()\n",
    "                    optimizer.step()\n",
    "\n",
    "                    # Print statistics\n",
    "                    running_loss += loss.item()\n",
    "\n",
    "                for i, data in enumerate(valDataloader, 0):\n",
    "                    net.eval()\n",
    "                    # Get the inputs and labels\n",
    "                    inputs, labels = data\n",
    "                    data=None\n",
    "                    inputs, labels = inputs.to(device), labels.to(device)\n",
    "\n",
    "                    # Forward pass\n",
    "                    outputs = net(inputs)\n",
    "                    val_totalAcc += torch.eq(torch.argmax(outputs, dim=1), labels).sum()\n",
    "\n",
    "                    loss = criterion(outputs, labels)\n",
    "\n",
    "                    # Print statistics\n",
    "                    val_running_loss += loss.item()\n",
    "\n",
    "                scheduler.step(val_running_loss)\n",
    "\n",
    "                trainAccList.append(totalAcc/len(trainDataset))\n",
    "                valAccList.append(val_totalAcc/len(valDataset))\n",
    "                trainLossList.append(running_loss/len(trainDataloader))\n",
    "                valLossList.append(val_running_loss/len(valDataloader))\n",
    "                print('[%d] train loss: %.3f train acc: %.3f val loss: %.3f val acc: %.3f '\n",
    "                      % (epoch + 1,trainLossList[epoch],  trainAccList[epoch], valLossList[epoch], valAccList[epoch]))\n",
    "                running_loss = 0.0\n",
    "                totalAcc = 0.0\n",
    "                val_running_loss = 0.0\n",
    "                val_totalAcc = 0.0\n",
    "                current_learning_rate = optimizer.param_groups[0]['lr']\n",
    "                # Break if totalAcc is above 0.95\n",
    "                if trainAccList[epoch] > 0.85 or current_learning_rate < 1e-6:\n",
    "                    print(\"Training accuracy reached above 0.95 or learning rate below 1e-6. Stopping training.\")\n",
    "                    break\n",
    "\n",
    "            print('Finished training')\n",
    "            df.save_training_data_to_file(df.transfer_to_cpu([learning_rate,weight_decay,patience]),df.transfer_to_cpu(trainLossList)\n",
    "                                  ,df.transfer_to_cpu(trainAccList),df.transfer_to_cpu(valLossList)\n",
    "                                  ,df.transfer_to_cpu(valAccList))\n",
    "print(\"Best Hyperparameters:\", best_hyperparameters)\n",
    "done=True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if done:\n",
    "\toutput.show() # displays captured output\n",
    "else:\n",
    "\tprint(\"cell above still seem to be running, wait some more..\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check the current GPU memory usage in bytes\n",
    "current_memory = torch.cuda.memory_allocated(device=device)\n",
    "print(f\"Current GPU memory usage: {current_memory / (1024**3):.2f} GB\")\n",
    "\n",
    "# Check the GPU memory reserved by PyTorch (memory that can be freed, but is not returned to the OS)\n",
    "reserved_memory = torch.cuda.memory_reserved(device=device)\n",
    "print(f\"GPU memory reserved by PyTorch: {reserved_memory / (1024**3):.2f} GB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.plot_loss_and_accuracy(df.transfer_to_cpu(trainLossList), df.transfer_to_cpu(trainAccList), df.transfer_to_cpu(valLossList), df.transfer_to_cpu(valAccList))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cm1, acc1 = df.create_confusion_matrix(trainDataloader, net, device=device)\n",
    "title1 = 'Confusion matrix for training set'\n",
    "\n",
    "cm2, acc2 = df.create_confusion_matrix(valDataloader, net, device=device)\n",
    "title2 = 'Confusion matrix for validation set'\n",
    "\n",
    "cm3, acc3 = df.create_confusion_matrix(testDataloader, net, device=device)\n",
    "title3 = 'Confusion matrix for test set'\n",
    "\n",
    "df.plot_3_confusion_matrices(cm1, cm2, cm3, acc1, acc2, acc3, num_classes, title1, title2, title3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# Example usage:\n",
    "# Assuming you have your confusion matrices and accuracies\n",
    "layer_sizes = [conv1toconv2, conv2toconv3, conv3toconv4, conv4toconv5, conv5toconv6, conv6tolin1]\n",
    "\n",
    "# Save the results to a file\n",
    "df.save_results_to_file(layer_sizes, cm1, acc1, cm2, acc2, cm3, acc3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
