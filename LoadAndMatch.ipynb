{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "## import numpy as np # linear algebra\n",
    "from matplotlib import pyplot as plt\n",
    "#We need these in this file:\n",
    "import sys\n",
    "sys.path.append('cropping')\n",
    "sys.path.append('scripts')\n",
    "import load_read_name_extractor as lrne\n",
    "import SVM_classifier_general as svm_general\n",
    "\n",
    "#Taget fra l√¶ngere nede i koden:\n",
    "import torch\n",
    "from torchvision import transforms\n",
    "from PIL import Image\n",
    "import random\n",
    "import itertools\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score\n",
    "import numpy as np\n",
    "import DeepFunctions as df\n",
    "from random import shuffle\n",
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader, random_split\n",
    "from sklearn.model_selection import train_test_split\n",
    "from torchvision import transforms\n",
    "from IPython.display import display\n",
    "import os\n",
    "import json\n",
    "import torch.optim as optim\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Remember to change the cuda:0 value. Should match a free GPU on the cluster or if using local probably just cuda:0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of available GPUs: 4\n",
      "GPU 0: NVIDIA TITAN X (Pascal)\n",
      "GPU 1: NVIDIA TITAN X (Pascal)\n",
      "GPU 2: NVIDIA TITAN X (Pascal)\n",
      "GPU 3: NVIDIA TITAN X (Pascal)\n",
      "cuda:0\n"
     ]
    }
   ],
   "source": [
    "# Call the function to list GPU names\n",
    "df.list_gpu_names()\n",
    "#R\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainDataloader = torch.load('train_loader.pth')\n",
    "valDataloader = torch.load('val_loader.pth')\n",
    "testDataloader = torch.load('test_loader.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "lenthdataset = torch.load('lengthsdataset.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'torch.utils.data.dataloader.DataLoader'>\n",
      "117\n",
      "<torch.utils.data.dataloader.DataLoader object at 0x7fecfb951dc0>\n"
     ]
    }
   ],
   "source": [
    "print(type(trainDataloader))\n",
    "print(len(trainDataloader))\n",
    "print(trainDataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#We have ignored class 9 since there were only 1 image with that class. Therfore we have 9 classes.\n",
    "num_classes = 9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResidualBlock(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, kernel_size=3,stride=1):\n",
    "        super(ResidualBlock, self).__init__()\n",
    "\n",
    "        # Define the convolutional layers\n",
    "        self.conv1 = nn.Conv2d(in_channels, out_channels, kernel_size=kernel_size, stride=stride, padding=1, bias=False)\n",
    "        self.bn1 = nn.BatchNorm2d(out_channels)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.conv2 = nn.Conv2d(out_channels, out_channels, kernel_size=kernel_size, stride=stride, padding=1, bias=False)\n",
    "        self.bn2 = nn.BatchNorm2d(out_channels)\n",
    "\n",
    "        # Shortcut connection (identity mapping)\n",
    "        self.shortcut = nn.Sequential()\n",
    "        if stride != 1 or in_channels != out_channels:\n",
    "            self.shortcut = nn.Sequential(\n",
    "                nn.Conv2d(in_channels, out_channels, kernel_size=1, stride=stride, bias=False),\n",
    "                nn.BatchNorm2d(out_channels)\n",
    "            )\n",
    "\n",
    "    def forward(self, x):\n",
    "        residual = x\n",
    "\n",
    "        out = self.conv1(x)\n",
    "        out = self.bn1(out)\n",
    "        out = self.relu(out)\n",
    "\n",
    "        out = self.conv2(out)\n",
    "        out = self.bn2(out)\n",
    "\n",
    "        # Add the shortcut connection\n",
    "        out += self.shortcut(residual)\n",
    "        out = self.relu(out)\n",
    "\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "kernel_size = 3\n",
    "kernel_size1 = 3\n",
    "padding = 0\n",
    "# define network\n",
    "conv1toconv2 = 16\n",
    "conv2toconv3 = 32\n",
    "conv3toconv4 = 64\n",
    "conv4toconv5 = 128\n",
    "conv5tores1 = 256\n",
    "res1tores2 = 256\n",
    "res2tores3 = 256\n",
    "res3tores4 = 256\n",
    "res4tores5 = 256\n",
    "res5tores6 = 256\n",
    "res6tolin1 = 256\n",
    "lin = 256\n",
    "momentum = 0.1\n",
    "\n",
    "class NeuralNetwork(nn.Module):\n",
    "    def __init__(self, dropout_rate=0.5):\n",
    "        super(NeuralNetwork, self).__init__()\n",
    "        \n",
    "        self.conv1 = nn.Conv2d(3, conv1toconv2, kernel_size, padding=padding)\n",
    "        self.bn1 = nn.BatchNorm2d(conv1toconv2)\n",
    "        self.conv2 = nn.Conv2d(conv1toconv2, conv2toconv3, kernel_size, padding=padding)\n",
    "        self.bn2 = nn.BatchNorm2d(conv2toconv3)\n",
    "        self.conv3 = nn.Conv2d(conv2toconv3, conv3toconv4, kernel_size, padding=padding)\n",
    "        self.bn3 = nn.BatchNorm2d(conv3toconv4)\n",
    "        self.conv4 = nn.Conv2d(conv3toconv4, conv4toconv5, kernel_size, padding=padding)\n",
    "        self.bn4 = nn.BatchNorm2d(conv4toconv5)\n",
    "        self.conv5 = nn.Conv2d(conv4toconv5, conv5tores1, kernel_size, padding=padding)\n",
    "        self.bn5 = nn.BatchNorm2d(conv5tores1)\n",
    "        \n",
    "        self.residual1 = ResidualBlock(conv5tores1, res1tores2)\n",
    "        self.residual2 = ResidualBlock(res1tores2, res2tores3)\n",
    "        self.residual3 = ResidualBlock(res2tores3, res3tores4)\n",
    "        self.residual4 = ResidualBlock(res4tores5, res4tores5)\n",
    "        self.residual5 = ResidualBlock(res4tores5, res5tores6)\n",
    "        self.residual6 = ResidualBlock(res5tores6, res6tolin1)\n",
    "        \n",
    "        self.pool = nn.AvgPool2d(2, 2)\n",
    "        self.poolmax = nn.MaxPool2d(2,2)\n",
    "        self.flatten = nn.Flatten()\n",
    "        self.FC1 = nn.Linear(6 * res6tolin1, lin)\n",
    "        self.bn7 = nn.BatchNorm1d(lin)\n",
    "        self.FC2 = nn.Linear(lin,lin)\n",
    "        self.bn8 = nn.BatchNorm1d(lin)\n",
    "        self.FC3 = nn.Linear(lin, num_classes)\n",
    "        self.bn9 = nn.BatchNorm1d(256)\n",
    "        self.dropout = nn.Dropout(dropout_rate)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.conv1(x)\n",
    "        x = self.bn1(x)\n",
    "        x = F.relu(x)\n",
    "        #print(x.shape)\n",
    "        x = self.poolmax(x)\n",
    "        \n",
    "        #print(x.shape)\n",
    "        x = self.conv2(x)\n",
    "        x = self.bn2(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.pool(x)\n",
    "        \n",
    "        #print(x.shape)\n",
    "        x = self.conv3(x)\n",
    "        x = self.bn3(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.pool(x)\n",
    "        \n",
    "                                  \n",
    "        x = self.conv4(x)\n",
    "        x = self.bn4(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.pool(x)\n",
    "        #print(x.shape)\n",
    "        \n",
    "        x = self.conv5(x)\n",
    "        x = self.bn5(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.pool(x)\n",
    "        #print(x.shape)\n",
    "        \n",
    "        x = self.residual1(x)\n",
    "        x = self.residual2(x)\n",
    "        x = self.residual3(x)\n",
    "        x = self.residual4(x)\n",
    "        x = self.residual5(x)\n",
    "        x = self.residual6(x)\n",
    "        \n",
    "        x = self.pool(x)\n",
    "        \n",
    "        x = self.flatten(x)\n",
    "        x = self.FC1(x)\n",
    "        x = self.bn7(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.dropout(x)\n",
    "        \n",
    "        x = self.FC2(x)\n",
    "        x = self.bn8(x)\n",
    "        x = F.relu(x)\n",
    "        x = self.dropout(x)\n",
    "        \n",
    "        x = self.FC3(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "start\n",
      "[1] train loss: 2.073 train acc: 0.276 val loss: 1.972 val acc: 0.335 \n",
      "[2] train loss: 1.988 train acc: 0.335 val loss: 1.946 val acc: 0.356 \n",
      "[3] train loss: 1.963 train acc: 0.344 val loss: 1.946 val acc: 0.349 \n",
      "[4] train loss: 1.946 train acc: 0.354 val loss: 1.897 val acc: 0.395 \n",
      "[5] train loss: 1.916 train acc: 0.369 val loss: 1.869 val acc: 0.389 \n",
      "[6] train loss: 1.898 train acc: 0.388 val loss: 1.902 val acc: 0.390 \n",
      "[7] train loss: 1.894 train acc: 0.394 val loss: 1.860 val acc: 0.406 \n",
      "[8] train loss: 1.887 train acc: 0.408 val loss: 1.855 val acc: 0.430 \n",
      "[9] train loss: 1.871 train acc: 0.414 val loss: 1.825 val acc: 0.438 \n",
      "[10] train loss: 1.863 train acc: 0.416 val loss: 1.834 val acc: 0.438 \n",
      "[11] train loss: 1.857 train acc: 0.421 val loss: 1.820 val acc: 0.444 \n",
      "[12] train loss: 1.855 train acc: 0.425 val loss: 1.817 val acc: 0.445 \n",
      "[13] train loss: 1.845 train acc: 0.432 val loss: 1.830 val acc: 0.437 \n",
      "[14] train loss: 1.840 train acc: 0.433 val loss: 1.817 val acc: 0.444 \n",
      "[15] train loss: 1.837 train acc: 0.442 val loss: 1.817 val acc: 0.450 \n",
      "[16] train loss: 1.837 train acc: 0.442 val loss: 1.842 val acc: 0.425 \n",
      "[17] train loss: 1.824 train acc: 0.454 val loss: 1.797 val acc: 0.469 \n",
      "[18] train loss: 1.807 train acc: 0.471 val loss: 1.781 val acc: 0.474 \n",
      "[19] train loss: 1.802 train acc: 0.471 val loss: 1.765 val acc: 0.501 \n",
      "[20] train loss: 1.799 train acc: 0.483 val loss: 1.782 val acc: 0.476 \n",
      "[21] train loss: 1.801 train acc: 0.476 val loss: 1.786 val acc: 0.466 \n",
      "[22] train loss: 1.789 train acc: 0.493 val loss: 1.790 val acc: 0.458 \n",
      "[23] train loss: 1.778 train acc: 0.496 val loss: 1.747 val acc: 0.501 \n",
      "[24] train loss: 1.778 train acc: 0.489 val loss: 1.741 val acc: 0.510 \n",
      "[25] train loss: 1.765 train acc: 0.517 val loss: 1.754 val acc: 0.496 \n",
      "[26] train loss: 1.756 train acc: 0.519 val loss: 1.737 val acc: 0.509 \n",
      "[27] train loss: 1.748 train acc: 0.527 val loss: 1.771 val acc: 0.487 \n",
      "[28] train loss: 1.740 train acc: 0.533 val loss: 1.727 val acc: 0.517 \n",
      "[29] train loss: 1.733 train acc: 0.542 val loss: 1.735 val acc: 0.508 \n",
      "[30] train loss: 1.733 train acc: 0.542 val loss: 1.709 val acc: 0.532 \n",
      "[31] train loss: 1.713 train acc: 0.552 val loss: 1.717 val acc: 0.528 \n",
      "[32] train loss: 1.709 train acc: 0.558 val loss: 1.707 val acc: 0.545 \n",
      "[33] train loss: 1.695 train acc: 0.566 val loss: 1.720 val acc: 0.532 \n",
      "[34] train loss: 1.696 train acc: 0.582 val loss: 1.721 val acc: 0.537 \n",
      "[35] train loss: 1.680 train acc: 0.597 val loss: 1.722 val acc: 0.548 \n",
      "[36] train loss: 1.662 train acc: 0.605 val loss: 1.709 val acc: 0.560 \n",
      "[37] train loss: 1.655 train acc: 0.607 val loss: 1.720 val acc: 0.544 \n",
      "[38] train loss: 1.631 train acc: 0.638 val loss: 1.701 val acc: 0.560 \n",
      "[39] train loss: 1.619 train acc: 0.648 val loss: 1.695 val acc: 0.562 \n",
      "[40] train loss: 1.611 train acc: 0.657 val loss: 1.728 val acc: 0.547 \n",
      "[41] train loss: 1.609 train acc: 0.664 val loss: 1.702 val acc: 0.578 \n",
      "[42] train loss: 1.565 train acc: 0.696 val loss: 1.686 val acc: 0.579 \n",
      "[43] train loss: 1.562 train acc: 0.700 val loss: 1.679 val acc: 0.587 \n",
      "[44] train loss: 1.527 train acc: 0.728 val loss: 1.677 val acc: 0.589 \n",
      "[45] train loss: 1.515 train acc: 0.741 val loss: 1.661 val acc: 0.598 \n",
      "[46] train loss: 1.485 train acc: 0.773 val loss: 1.655 val acc: 0.616 \n",
      "[47] train loss: 1.476 train acc: 0.774 val loss: 1.652 val acc: 0.617 \n",
      "[48] train loss: 1.462 train acc: 0.783 val loss: 1.659 val acc: 0.602 \n",
      "[49] train loss: 1.429 train acc: 0.806 val loss: 1.654 val acc: 0.633 \n",
      "[50] train loss: 1.414 train acc: 0.825 val loss: 1.656 val acc: 0.629 \n",
      "[51] train loss: 1.398 train acc: 0.840 val loss: 1.672 val acc: 0.615 \n",
      "[52] train loss: 1.379 train acc: 0.853 val loss: 1.650 val acc: 0.634 \n",
      "[53] train loss: 1.349 train acc: 0.877 val loss: 1.680 val acc: 0.608 \n",
      "[54] train loss: 1.350 train acc: 0.882 val loss: 1.638 val acc: 0.642 \n",
      "[55] train loss: 1.342 train acc: 0.884 val loss: 1.668 val acc: 0.628 \n",
      "[56] train loss: 1.324 train acc: 0.901 val loss: 1.668 val acc: 0.633 \n",
      "[57] train loss: 1.299 train acc: 0.917 val loss: 1.630 val acc: 0.657 \n",
      "[58] train loss: 1.288 train acc: 0.926 val loss: 1.625 val acc: 0.654 \n",
      "[59] train loss: 1.283 train acc: 0.928 val loss: 1.639 val acc: 0.651 \n",
      "[60] train loss: 1.270 train acc: 0.942 val loss: 1.629 val acc: 0.663 \n",
      "[61] train loss: 1.255 train acc: 0.950 val loss: 1.627 val acc: 0.657 \n",
      "[62] train loss: 1.253 train acc: 0.953 val loss: 1.646 val acc: 0.653 \n",
      "[63] train loss: 1.244 train acc: 0.957 val loss: 1.616 val acc: 0.665 \n",
      "[64] train loss: 1.242 train acc: 0.959 val loss: 1.632 val acc: 0.660 \n",
      "[65] train loss: 1.235 train acc: 0.968 val loss: 1.630 val acc: 0.673 \n",
      "[66] train loss: 1.235 train acc: 0.961 val loss: 1.616 val acc: 0.671 \n",
      "[67] train loss: 1.227 train acc: 0.967 val loss: 1.637 val acc: 0.658 \n",
      "[68] train loss: 1.215 train acc: 0.976 val loss: 1.624 val acc: 0.667 \n",
      "Epoch 00069: reducing learning rate of group 0 to 8.0000e-05.\n",
      "[69] train loss: 1.219 train acc: 0.975 val loss: 1.631 val acc: 0.662 \n",
      "[70] train loss: 1.195 train acc: 0.987 val loss: 1.591 val acc: 0.688 \n",
      "[71] train loss: 1.189 train acc: 0.992 val loss: 1.584 val acc: 0.696 \n",
      "[72] train loss: 1.185 train acc: 0.995 val loss: 1.574 val acc: 0.702 \n",
      "[73] train loss: 1.183 train acc: 0.995 val loss: 1.574 val acc: 0.695 \n",
      "[74] train loss: 1.180 train acc: 0.997 val loss: 1.576 val acc: 0.696 \n",
      "[75] train loss: 1.183 train acc: 0.996 val loss: 1.581 val acc: 0.697 \n",
      "[76] train loss: 1.179 train acc: 0.996 val loss: 1.567 val acc: 0.704 \n",
      "[77] train loss: 1.179 train acc: 0.997 val loss: 1.570 val acc: 0.704 \n",
      "[78] train loss: 1.179 train acc: 0.996 val loss: 1.570 val acc: 0.704 \n",
      "[79] train loss: 1.177 train acc: 0.998 val loss: 1.568 val acc: 0.708 \n",
      "[80] train loss: 1.177 train acc: 0.998 val loss: 1.573 val acc: 0.701 \n",
      "[81] train loss: 1.179 train acc: 0.996 val loss: 1.565 val acc: 0.704 \n",
      "[82] train loss: 1.176 train acc: 0.998 val loss: 1.572 val acc: 0.701 \n",
      "[83] train loss: 1.177 train acc: 0.998 val loss: 1.563 val acc: 0.711 \n",
      "[84] train loss: 1.174 train acc: 0.999 val loss: 1.574 val acc: 0.701 \n",
      "[85] train loss: 1.174 train acc: 0.998 val loss: 1.567 val acc: 0.704 \n",
      "[86] train loss: 1.176 train acc: 0.998 val loss: 1.570 val acc: 0.694 \n",
      "[87] train loss: 1.175 train acc: 0.998 val loss: 1.565 val acc: 0.704 \n",
      "[88] train loss: 1.173 train acc: 0.999 val loss: 1.566 val acc: 0.704 \n",
      "Epoch 00089: reducing learning rate of group 0 to 8.0000e-06.\n",
      "[89] train loss: 1.175 train acc: 0.998 val loss: 1.566 val acc: 0.706 \n",
      "[90] train loss: 1.174 train acc: 0.999 val loss: 1.563 val acc: 0.707 \n",
      "[91] train loss: 1.175 train acc: 0.999 val loss: 1.560 val acc: 0.712 \n",
      "[92] train loss: 1.171 train acc: 0.999 val loss: 1.562 val acc: 0.707 \n",
      "[93] train loss: 1.174 train acc: 0.998 val loss: 1.569 val acc: 0.704 \n",
      "[94] train loss: 1.172 train acc: 0.999 val loss: 1.563 val acc: 0.702 \n",
      "[95] train loss: 1.177 train acc: 0.998 val loss: 1.562 val acc: 0.712 \n",
      "[96] train loss: 1.171 train acc: 0.999 val loss: 1.557 val acc: 0.712 \n",
      "[97] train loss: 1.175 train acc: 0.998 val loss: 1.566 val acc: 0.706 \n",
      "[98] train loss: 1.172 train acc: 0.999 val loss: 1.558 val acc: 0.713 \n",
      "[99] train loss: 1.173 train acc: 0.999 val loss: 1.554 val acc: 0.712 \n",
      "[100] train loss: 1.171 train acc: 0.999 val loss: 1.558 val acc: 0.713 \n",
      "[101] train loss: 1.171 train acc: 0.999 val loss: 1.562 val acc: 0.708 \n",
      "[102] train loss: 1.173 train acc: 0.998 val loss: 1.561 val acc: 0.709 \n",
      "[103] train loss: 1.172 train acc: 0.998 val loss: 1.562 val acc: 0.711 \n",
      "[104] train loss: 1.172 train acc: 0.998 val loss: 1.564 val acc: 0.712 \n",
      "Epoch 00105: reducing learning rate of group 0 to 8.0000e-07.\n",
      "[105] train loss: 1.171 train acc: 0.999 val loss: 1.563 val acc: 0.709 \n",
      "[106] train loss: 1.170 train acc: 0.999 val loss: 1.559 val acc: 0.714 \n",
      "[107] train loss: 1.171 train acc: 0.999 val loss: 1.564 val acc: 0.702 \n",
      "[108] train loss: 1.173 train acc: 0.998 val loss: 1.562 val acc: 0.709 \n",
      "[109] train loss: 1.172 train acc: 0.999 val loss: 1.570 val acc: 0.706 \n",
      "[110] train loss: 1.170 train acc: 1.000 val loss: 1.564 val acc: 0.706 \n",
      "Epoch 00111: reducing learning rate of group 0 to 8.0000e-08.\n",
      "[111] train loss: 1.176 train acc: 0.997 val loss: 1.569 val acc: 0.707 \n",
      "Training accuracy reached above 0.95 or learning rate below 1e-6. Stopping training.\n",
      "Finished training\n",
      "start\n",
      "[1] train loss: 2.062 train acc: 0.285 val loss: 1.958 val acc: 0.358 \n",
      "[2] train loss: 1.985 train acc: 0.324 val loss: 1.920 val acc: 0.366 \n",
      "[3] train loss: 1.956 train acc: 0.357 val loss: 1.930 val acc: 0.368 \n",
      "[4] train loss: 1.930 train acc: 0.364 val loss: 1.890 val acc: 0.381 \n",
      "[5] train loss: 1.913 train acc: 0.388 val loss: 1.872 val acc: 0.415 \n",
      "[6] train loss: 1.888 train acc: 0.395 val loss: 1.859 val acc: 0.420 \n",
      "[7] train loss: 1.885 train acc: 0.403 val loss: 1.832 val acc: 0.434 \n",
      "[8] train loss: 1.875 train acc: 0.416 val loss: 1.903 val acc: 0.385 \n",
      "[9] train loss: 1.868 train acc: 0.419 val loss: 1.911 val acc: 0.382 \n",
      "[10] train loss: 1.865 train acc: 0.422 val loss: 1.836 val acc: 0.438 \n",
      "[11] train loss: 1.859 train acc: 0.421 val loss: 1.829 val acc: 0.435 \n",
      "[12] train loss: 1.842 train acc: 0.438 val loss: 1.844 val acc: 0.437 \n",
      "[13] train loss: 1.844 train acc: 0.435 val loss: 1.833 val acc: 0.429 \n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_3087062/2016698281.py\u001b[0m in \u001b[0;36m<cell line: 28>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     72\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     73\u001b[0m                         \u001b[0;31m# Print statistics\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 74\u001b[0;31m                         \u001b[0mrunning_loss\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     75\u001b[0m                         \u001b[0;32mdel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     76\u001b[0m                         \u001b[0;32mdel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlabels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "#%%capture output\n",
    "done=False\n",
    "learning_rates = [0.0008]\n",
    "patience_values = [5]\n",
    "weight_decay_values =[0, 0.00005]\n",
    "label_smoothing_values =[0.3]\n",
    "message = \"Standard\"\n",
    "best_val_acc = 0.0\n",
    "best_hyperparameters = {}\n",
    "\n",
    "# Set the number of epochs\n",
    "num_epochs = 140\n",
    "totalAcc = 0\n",
    "\n",
    "trainAccList = []\n",
    "valAccList = []\n",
    "trainLossList = []\n",
    "valLossList = []\n",
    "\n",
    "# Variables for early stopping\n",
    "best_val_accuracy = 0.0\n",
    "patience = 140  # Number of epochs to wait for improvement\n",
    "counter = 0  # Counter for consecutive epochs without improvement\n",
    "best_epoch = 0\n",
    "best_model_state = None\n",
    "\n",
    "# Training loop + tuning\n",
    "for learning_rate in learning_rates:\n",
    "    for weight_decay in weight_decay_values:\n",
    "        for patience in patience_values:\n",
    "            for label_smoothing in label_smoothing_values:\n",
    "                net = NeuralNetwork()\n",
    "                print(\"start\")\n",
    "                net.to(device)\n",
    "                #print(net)\n",
    "                #learning_rate = 0.001\n",
    "                # Define the optimizer\n",
    "                criterion = nn.CrossEntropyLoss(label_smoothing=label_smoothing).to(device)\n",
    "                optimizer = optim.Adam(net.parameters(), lr=learning_rate, weight_decay=weight_decay)\n",
    "                scheduler = ReduceLROnPlateau(optimizer, 'min', patience=patience, verbose=True)\n",
    "\n",
    "                trainAccList = []\n",
    "                valAccList = []\n",
    "                trainLossList = []\n",
    "                valLossList = []\n",
    "                # Training loop\n",
    "                for epoch in range(num_epochs):\n",
    "                    running_loss = 0.0\n",
    "                    totalAcc = 0.0\n",
    "                    val_running_loss = 0.0\n",
    "                    val_totalAcc = 0.0\n",
    "\n",
    "                    for i, data in enumerate(trainDataloader, 0):\n",
    "                        net.train()\n",
    "                        # Get the inputs and labels\n",
    "                        inputs, labels = data\n",
    "\n",
    "                        inputs, labels = inputs.to(device), labels.to(device)\n",
    "\n",
    "                        # Zero the parameter gradients\n",
    "                        optimizer.zero_grad()\n",
    "\n",
    "                        # Forward pass\n",
    "                        outputs = net(inputs)\n",
    "                        totalAcc += torch.eq(torch.argmax(outputs, dim=1), labels).sum()\n",
    "\n",
    "                        loss = criterion(outputs, labels)\n",
    "\n",
    "                        # Backward pass and optimization\n",
    "                        loss.backward()\n",
    "                        optimizer.step()\n",
    "\n",
    "                        # Print statistics\n",
    "                        running_loss += loss.item()\n",
    "                        del(inputs)\n",
    "                        del(labels)\n",
    "\n",
    "                    for i, data in enumerate(valDataloader, 0):\n",
    "                        net.eval()\n",
    "                        # Get the inputs and labels\n",
    "                        inputs, labels = data\n",
    "                        data=None\n",
    "                        inputs, labels = inputs.to(device), labels.to(device)\n",
    "\n",
    "                        # Forward pass\n",
    "                        outputs = net(inputs)\n",
    "                        val_totalAcc += torch.eq(torch.argmax(outputs, dim=1), labels).sum()\n",
    "\n",
    "                        loss = criterion(outputs, labels)\n",
    "\n",
    "                        # Print statistics\n",
    "                        val_running_loss += loss.item()\n",
    "                        del(inputs)\n",
    "                        del(labels)\n",
    "\n",
    "                    scheduler.step(val_running_loss)\n",
    "\n",
    "                    trainAccList.append(totalAcc/lenthdataset[0])\n",
    "                    valAccList.append(val_totalAcc/lenthdataset[1])\n",
    "                    trainLossList.append(running_loss/len(trainDataloader))\n",
    "                    valLossList.append(val_running_loss/len(valDataloader))\n",
    "                    print('[%d] train loss: %.3f train acc: %.3f val loss: %.3f val acc: %.3f '\n",
    "                          % (epoch + 1,trainLossList[epoch],  trainAccList[epoch], valLossList[epoch], valAccList[epoch]))\n",
    "                    running_loss = 0.0\n",
    "                    totalAcc = 0.0\n",
    "                    val_running_loss = 0.0\n",
    "                    val_totalAcc = 0.0\n",
    "                    current_learning_rate = optimizer.param_groups[0]['lr']\n",
    "\n",
    "                    # Check for improvement in validation accuracy\n",
    "\n",
    "                    if valAccList[epoch] > best_val_acc and valAccList[epoch] > 0.6:\n",
    "                        best_val_acc = valAccList[epoch]\n",
    "                        best_model_state = net.state_dict()\n",
    "                        best_epoch = epoch\n",
    "\n",
    "                    # Break if totalAcc is above 0.95\n",
    "                    if trainAccList[epoch] > 1 or current_learning_rate < 1e-7:\n",
    "                        print(\"Training accuracy reached above 0.95 or learning rate below 1e-6. Stopping training.\")\n",
    "                        break\n",
    "\n",
    "                if best_model_state is not None:\n",
    "                    net.load_state_dict(best_model_state)\n",
    "                    torch.save(best_model_state, 'models/best_model_stateAcc%.3fepoch%.1f.pt' % (best_val_acc, best_epoch))\n",
    "                    filename = 'models/best_model_stateAcc%.3fepoch%.1f.pt' % (best_val_acc, best_epoch)\n",
    "                best_val_accuracy = 0.0\n",
    "                best_epoch = 0\n",
    "                best_model_state = None\n",
    "                    \n",
    "\n",
    "                print('Finished training')\n",
    "                df.save_training_data_to_file(df.transfer_to_cpu([learning_rate,weight_decay,patience,momentum,label_smoothing,filename]),df.transfer_to_cpu(trainLossList)\n",
    "                                                  ,df.transfer_to_cpu(trainAccList),df.transfer_to_cpu(valLossList)\n",
    "                                                  ,df.transfer_to_cpu(valAccList))\n",
    "done=True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if done:\n",
    "\toutput.show() # displays captured output\n",
    "else:\n",
    "\tprint(\"cell above still seem to be running, wait some more..\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check the current GPU memory usage in bytes\n",
    "current_memory = torch.cuda.memory_allocated(device=device)\n",
    "print(f\"Current GPU memory usage: {current_memory / (1024**3):.2f} GB\")\n",
    "\n",
    "# Check the GPU memory reserved by PyTorch (memory that can be freed, but is not returned to the OS)\n",
    "reserved_memory = torch.cuda.memory_reserved(device=device)\n",
    "print(f\"GPU memory reserved by PyTorch: {reserved_memory / (1024**3):.2f} GB\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
